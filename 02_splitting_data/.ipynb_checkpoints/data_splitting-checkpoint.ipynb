{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d379f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd0f1de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ext smiles 65 tox and 25 nontox for this dataset  cardiotox_with_sider_inactives\n",
      "1    65\n",
      "0    25\n",
      "Name: Cardiotox (with SIDER inactives), dtype: int64\n",
      "Using ext smiles 65 tox and 25 nontox for this dataset  cardiotox_with_sider_actives\n",
      "1    65\n",
      "0    25\n",
      "Name: Cardiotox (with SIDER actives), dtype: int64\n",
      "Using ext smiles 65 tox and 25 nontox for this dataset  cardiotox_with_sider_all\n",
      "1    65\n",
      "0    25\n",
      "Name: Cardiotox (with SIDER all), dtype: int64\n",
      "Using 10% train test stratified split for this dataset  sider_cardiacdisorders\n",
      "1    93\n",
      "0    40\n",
      "Name: Cardiac disorders, dtype: int64\n",
      "Using ext smiles 65 tox and 25 nontox for this dataset  DICTrank\n",
      "1    65\n",
      "0    25\n",
      "Name: DICTrank, dtype: int64\n",
      "cardiotox_with_sider_inactives\n",
      "sider\n",
      "cardiotox_with_sider_actives\n",
      "cardiotox_with_sider_all\n",
      "sider_cardiacdisorders\n",
      "DICTrank\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ss2686/03_DICTrank')\n",
    "\n",
    "from scripts.stratified_split_helper import stratified_data_split\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Loading the dictionary\n",
    "\n",
    "pickle_file_path = '../01_standardise_datasets/activity_columns_mapping_selected.pkl'\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Specify the file path to the pickle file\n",
    "file_ext_DICT_path_pkl = \"smiles_list_ext_DICT_test.pkl\"  # Replace \"your_file.pkl\" with the actual file path\n",
    "\n",
    "# Load the data from the pickle file into a list\n",
    "with open(file_ext_DICT_path_pkl, \"rb\") as file:\n",
    "    smiles_list_ext_DICT = pickle.load(file)\n",
    "     \n",
    "    # Loading the dictionary\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    activity_columns_mapping = pickle.load(file)\n",
    "    \n",
    "def process_datasets(directory='../data/binarised/'):\n",
    "    datasets = {}\n",
    "    splits = {}\n",
    "    \n",
    "    # Load datasets from given directory\n",
    "    for foldername in os.listdir(directory):\n",
    "        \n",
    "        if not foldername.startswith('.'):  # Ignore folders starting with a dot\n",
    "            \n",
    "            #print(foldername)\n",
    "            file_path = os.path.join(directory, foldername, f\"{foldername}_binarised.csv.gz\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                datasets[foldername] = pd.read_csv(file_path, compression='gzip')\n",
    "            else:\n",
    "                print(f\"No matching file found for folder: {foldername}\")\n",
    "\n",
    "    # Split for each dataset and each activity column\n",
    "    for name, df in datasets.items():\n",
    "        activity_cols = activity_columns_mapping.get(name, [])\n",
    "        dataset_splits = {}\n",
    "        \n",
    "        for col in activity_cols:\n",
    "            if name in [\"cardiotox_with_sider_inactives\", \"cardiotox_with_sider_actives\", \n",
    "                         \"cardiotox_with_sider_all\", \"DICTrank\"] and smiles_list_ext_DICT:\n",
    "                \n",
    "                print(\"Using ext smiles 65 tox and 25 nontox for this dataset \", name)\n",
    "                test_df = df[df['Standardized_SMILES'].isin(smiles_list_ext_DICT)]\n",
    "                print(test_df[col].value_counts())\n",
    "                \n",
    "                train_df = df[~df['Standardized_SMILES'].isin(smiles_list_ext_DICT)]\n",
    "                \n",
    "            else:\n",
    "                print(\"Using 10% train test stratified split for this dataset \", name)\n",
    "                \n",
    "                train_df, test_df = stratified_data_split(df, activity_col=col, dataset_name=name)\n",
    "                print(test_df[col].value_counts())\n",
    "                \n",
    "            # Filter the columns for saving\n",
    "            cols_to_keep = ['Standardized_SMILES', 'Standardized_InChI', col]\n",
    "            train_df = train_df[cols_to_keep]\n",
    "            test_df = test_df[cols_to_keep]\n",
    "            \n",
    "            dataset_splits[col] = {'train': train_df, 'test': test_df}\n",
    "\n",
    "        splits[name] = dataset_splits\n",
    "    \n",
    "    # Save the splits to new directories\n",
    "    output_dir = \"../data/processed_binarised__splits/\"\n",
    "    # Ensure the directory exists, if not, create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Iterate through each dataset name, activities, and their corresponding splits\n",
    "    for dataset_name, activities in splits.items():\n",
    "        print(dataset_name)\n",
    "        for activity, split_data in activities.items():\n",
    "            for data_type, split_df in split_data.items():\n",
    "                # Create a path for the dataset if it doesn't exist\n",
    "                dataset_path = os.path.join(output_dir, dataset_name)\n",
    "                if not os.path.exists(dataset_path):\n",
    "                    os.makedirs(dataset_path)\n",
    "\n",
    "                # Define the full output path for the split dataframe\n",
    "                output_path = os.path.join(dataset_path, f\"{activity}_{data_type}.csv.gz\")\n",
    "\n",
    "                # Save the dataframe\n",
    "                split_df.to_csv(output_path, index=False, compression='gzip')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d3722e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardiotox_with_sider_inactives\n",
      "['Cardiotox (with SIDER inactives)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activities: 100%|█████████████████████| 1/1 [00:00<00:00, 135.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1163\n",
      "1    677\n",
      "0    486\n",
      "Name: Cardiotox (with SIDER inactives), dtype: int64\n",
      "90\n",
      "1    65\n",
      "0    25\n",
      "Name: Cardiotox (with SIDER inactives), dtype: int64\n",
      "cardiotox_with_sider_actives\n",
      "['Cardiotox (with SIDER actives)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activities: 100%|█████████████████████| 1/1 [00:00<00:00, 129.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243\n",
      "1    990\n",
      "0    253\n",
      "Name: Cardiotox (with SIDER actives), dtype: int64\n",
      "90\n",
      "1    65\n",
      "0    25\n",
      "Name: Cardiotox (with SIDER actives), dtype: int64\n",
      "cardiotox_with_sider_all\n",
      "['Cardiotox (with SIDER all)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activities: 100%|█████████████████████| 1/1 [00:00<00:00, 119.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476\n",
      "1    990\n",
      "0    486\n",
      "Name: Cardiotox (with SIDER all), dtype: int64\n",
      "90\n",
      "1    65\n",
      "0    25\n",
      "Name: Cardiotox (with SIDER all), dtype: int64\n",
      "sider_cardiacdisorders\n",
      "['Cardiac disorders']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activities: 100%|█████████████████████| 1/1 [00:00<00:00, 133.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1189\n",
      "1    829\n",
      "0    360\n",
      "Name: Cardiac disorders, dtype: int64\n",
      "133\n",
      "1    93\n",
      "0    40\n",
      "Name: Cardiac disorders, dtype: int64\n",
      "DICTrank\n",
      "['DICTrank']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activities: 100%|█████████████████████| 1/1 [00:00<00:00, 130.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n",
      "1    677\n",
      "0    253\n",
      "Name: DICTrank, dtype: int64\n",
      "90\n",
      "1    65\n",
      "0    25\n",
      "Name: DICTrank, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data_path = '../data/processed_binarised__splits/'\n",
    "\n",
    "\n",
    "for dataset in os.listdir(data_path):\n",
    "\n",
    "    # Exclude hidden files or directories like .ipynb_checkpoints\n",
    "    if dataset.startswith('.'):\n",
    "        continue\n",
    "    print(dataset)\n",
    "\n",
    "    # Get all the file names for this dataset\n",
    "    all_files = os.listdir(os.path.join(data_path, dataset))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Extract activity names by removing the _train.csv.gz or _test.csv.gz from file names\n",
    "    activity_names = list(set([f.replace(\"_train.csv.gz\", \"\").replace(\"_test.csv.gz\", \"\")  for f in all_files if not f.startswith(\".ipynb_checkpoints\")]))\n",
    "\n",
    "    print(activity_names)\n",
    "    \n",
    "    for activity in tqdm(activity_names, desc=\"Processing activities\"):\n",
    "        \n",
    "        train_path = os.path.join(data_path, dataset, f\"{activity}_train.csv.gz\")\n",
    "        test_path = os.path.join(data_path, dataset, f\"{activity}_test.csv.gz\")\n",
    "\n",
    "        train_df = pd.read_csv(train_path, compression='gzip')\n",
    "        test_df = pd.read_csv(test_path, compression='gzip')\n",
    "        \n",
    "        print(len(train_df))\n",
    "        print(train_df[activity].value_counts())\n",
    "        print(len(test_df))\n",
    "        print(test_df[activity].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca8ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
